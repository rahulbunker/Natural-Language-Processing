{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7922a11c-0200-4c75-bdbe-3e86821455ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c371e8-f24e-40f1-9eb5-d14a6991a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _ , filenames in os.walk('kaggle/input'):\n",
    "    for filename in filename:\n",
    "        print(os.path.join(dirname,filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5019196-63a4-4524-befa-87bf2ce7fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\OneDrive\\\\Desktop\\\\pandas\\\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c3cfb4-7020-47a6-a2b5-ec6898816201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74deec6-0578-4872-97ac-cec6082192da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382210dc-45e2-493e-a663-587ee0dd1d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d7bad0-2c99-473e-b797-6b45ad4c0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"]=df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67d74e1-cc0a-4976-8b4a-fe8cce359eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "# all review column data chnage in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84ceca3-077b-4c25-93d3-6300f5297b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"]=df[\"sentiment\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2957b25b-e37f-4215-a85e-66a0b6a3f0f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  POSITIVE\n",
       "1      a wonderful little production. <br /><br />the...  POSITIVE\n",
       "2      i thought this was a wonderful way to spend ti...  POSITIVE\n",
       "3      basically there's a family where a little boy ...  NEGATIVE\n",
       "4      petter mattei's \"love in the time of money\" is...  POSITIVE\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  POSITIVE\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  NEGATIVE\n",
       "49997  i am a catholic taught in parochial elementary...  NEGATIVE\n",
       "49998  i'm going to have to disagree with the previou...  NEGATIVE\n",
       "49999  no one expects the star trek movies to be high...  NEGATIVE\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92a62184-2ba8-4de0-865b-bab4b0c5b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment\"]=df[\"sentiment\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563aa5b0-96ff-454d-a044-dce3ed7f4e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>i'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expects the star trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      one of the other reviewers has mentioned that ...  positive\n",
       "1      a wonderful little production. <br /><br />the...  positive\n",
       "2      i thought this was a wonderful way to spend ti...  positive\n",
       "3      basically there's a family where a little boy ...  negative\n",
       "4      petter mattei's \"love in the time of money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  i thought this movie did a down right good job...  positive\n",
       "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  i am a catholic taught in parochial elementary...  negative\n",
       "49998  i'm going to have to disagree with the previou...  negative\n",
       "49999  no one expects the star trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21b388-b501-4ded-8fb5-5e0113b6925c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcaf9027-bbb7-48c6-a6ad-d6014675c090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Yeh <strong>bold</strong> text hai,  oh hello mr. jatin yeh <em>italic</em> hai, aur yeh <u>underline</u> kiya hua hai.</p>\n"
     ]
    }
   ],
   "source": [
    "x=\"<p>Yeh <strong>bold</strong> text hai,  oh hello mr. jatin yeh <em>italic</em> hai, aur yeh <u>underline</u> kiya hua hai.</p>\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f124e5-88c3-4f9b-9945-0613fe3859c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeh bold text hai,  oh hello mr. jatin yeh italic hai, aur yeh underline kiya hua hai.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "html_text = x\n",
    "\n",
    "# HTML tags remove karne ke liye regex\n",
    "clean_text = re.sub(r'<.*?>', '', html_text)\n",
    "\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36065898-0e4f-43e7-9bb4-dd3b51673401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags_regex(text):\n",
    "    clean_text = re.sub(r'<.*?>', '', text)\n",
    "    return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f9c3f4c-c8b8-4059-9703-4f1da72d6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"]=df[\"review\"].apply(remove_html_tags_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e3bc367-96f2-4b09-806a-f31fb3f08e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      one of the other reviewers has mentioned that ...  positive\n",
      "1      a wonderful little production. the filming tec...  positive\n",
      "2      i thought this was a wonderful way to spend ti...  positive\n",
      "3      basically there's a family where a little boy ...  negative\n",
      "4      petter mattei's \"love in the time of money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  i thought this movie did a down right good job...  positive\n",
      "49996  bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  i am a catholic taught in parochial elementary...  negative\n",
      "49998  i'm going to have to disagree with the previou...  negative\n",
      "49999  no one expects the star trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df) # all html tag are remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b559b6d5-57a6-4ee3-8b49-d9fbfb168d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701121e5-8aec-4616-ae80-352256715f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45d828f9-b4d3-4a95-9d38-efab5b7fd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = \"Visit Google at https://www.google.com for searching.\"\n",
    "x2 = \"Checkout our site: https://www.example.com for more info.\"\n",
    "x3 = \"Download files from https://files.example.com/data.zip anytime.\"\n",
    "x4 = \"Contact us via https://contact.example.com or call us.\"\n",
    "x5 = \"Follow updates on https://twitter.com/exampleprofile daily.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e29ee8f-fe77-4431-acd2-824cfbb3b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', text).strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a7e707-d647-4519-827b-be3bc727520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visit Google at  for searching.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbd09ee0-e8cd-42bf-9422-bf138bac3050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Checkout our site:  for more info.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d89a0bfc-ee36-47c2-be19-d2123de82e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Follow updates on  daily.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_urls(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf9f37-6d1d-4093-a033-2d492b000eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00cc567b-f30c-4672-ac51-0c281fe035c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6965a6c7-d923-47c6-9506-c4a46e043280",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdae20ee-214f-4cb5-beec-a4a8ca3a2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char  in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text\n",
    "\n",
    " #  this function slow next function more fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d11b41c-5839-4204-899a-f69366e65b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello mr rahul ji  how are you  \n"
     ]
    }
   ],
   "source": [
    "print(remove_punc(\"hello! mr. rahul ji , how are you ? \")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b0f8a5e-8c07-4a42-9834-7652bb4b851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"hello! mr. rahul ji , how are you ? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d470476e-a848-4b49-924d-ae2b04f0a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello mr rahul ji  how are you  \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(remove_punc(x)) \n",
    "time1=time.time() - start\n",
    "print(time1*5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cffbb-6ec7-478a-97ef-7669b75aa9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e91c7d63-379b-4048-a19b-f4587272844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "# this code fast 18x from first pucctutation function \n",
    "# yahi code use krna hai hmesha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "285fe308-9a13-40e1-8a97-8f494a35033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello mr rahul ji  how are you  \n"
     ]
    }
   ],
   "source": [
    "print(remove_punc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2955666f-57a5-4d0e-9c0e-f89611ee1f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello mr rahul ji  how are you  \n",
      "50.34208297729492\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc1(x))\n",
    "time2=time.time()-start\n",
    "print(time2*50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68539ab6-ad70-4e15-afac-234ca055d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2618facb-54d1-46a4-b496-17ba075c0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words= {\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"ADIH\": \"Another Day In Hell\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"BAE\": \"Before Anyone Else\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRUH\": \"Bro\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BSAAW\": \"Big Smile And A Wink\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"BWL\": \"Bursting With Laughter\",\n",
    "    \"CSL\": \"Canâ€™t Stop Laughing\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"DM\": \"Direct Message\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FIMH\": \"Forever In My Heart\",\n",
    "    \"FOMO\": \"Fear Of Missing Out\",\n",
    "    \"FR\": \"For Real\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYP\": \"For You Page\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GOAT\": \"Greatest Of All Time\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"HBD\": \"Happy Birthday\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek You\",\n",
    "    \"IDC\": \"I Donâ€™t Care\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"IFYP\": \"I Feel Your Pain\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"ILY\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMU\": \"I Miss You\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"IYKYK\": \"If You Know, You Know\",\n",
    "    \"JK\": \"Just Kidding\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"L\": \"Loss\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMK\": \"Let Me Know\",\n",
    "    \"LMAO\": \"Laughing My A** Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"MFW\": \"My Face When\",\n",
    "    \"MID\": \"Mediocre\",\n",
    "    \"MRW\": \"My Reaction When\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"NVM\": \"Never Mind\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"NPC\": \"Non-Player Character\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"OP\": \"Overpowered\",\n",
    "    \"PITA\": \"Pain In The A**\",\n",
    "    \"POV\": \"Point Of View\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n",
    "    \"RN\": \"Right Now\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your Sex And Age\",\n",
    "    \"SUS\": \"Suspicious\",\n",
    "    \"TBH\": \"To Be Honest\",\n",
    "    \"TFW\": \"That Feeling When\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TIME\": \"Tears In My Eyes\",\n",
    "    \"TLDR\": \"Too Long, Didnâ€™t Read\",\n",
    "    \"TNTL\": \"Trying Not To Laugh\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"W\": \"Win\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F**k\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"WYD\": \"What You Doing?\",\n",
    "    \"WYWH\": \"Wish You Were Here\",\n",
    "    \"ZZZ\": \"Sleeping, Bored, Tired\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84f324d4-35d3-40ec-bf5e-1097fa1eccd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A3': 'Anytime, Anywhere, Anyplace',\n",
       " 'ADIH': 'Another Day In Hell',\n",
       " 'AFK': 'Away From Keyboard',\n",
       " 'AFAIK': 'As Far As I Know',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'ASL': 'Age, Sex, Location',\n",
       " 'ATK': 'At The Keyboard',\n",
       " 'ATM': 'At The Moment',\n",
       " 'BAE': 'Before Anyone Else',\n",
       " 'BAK': 'Back At Keyboard',\n",
       " 'BBL': 'Be Back Later',\n",
       " 'BBS': 'Be Back Soon',\n",
       " 'BFN': 'Bye For Now',\n",
       " 'B4N': 'Bye For Now',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BRUH': 'Bro',\n",
       " 'BRT': 'Be Right There',\n",
       " 'BSAAW': 'Big Smile And A Wink',\n",
       " 'BTW': 'By The Way',\n",
       " 'BWL': 'Bursting With Laughter',\n",
       " 'CSL': 'Canâ€™t Stop Laughing',\n",
       " 'CU': 'See You',\n",
       " 'CUL8R': 'See You Later',\n",
       " 'CYA': 'See You',\n",
       " 'DM': 'Direct Message',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'FC': 'Fingers Crossed',\n",
       " 'FIMH': 'Forever In My Heart',\n",
       " 'FOMO': 'Fear Of Missing Out',\n",
       " 'FR': 'For Real',\n",
       " 'FWIW': \"For What It's Worth\",\n",
       " 'FYP': 'For You Page',\n",
       " 'FYI': 'For Your Information',\n",
       " 'G9': 'Genius',\n",
       " 'GAL': 'Get A Life',\n",
       " 'GG': 'Good Game',\n",
       " 'GMTA': 'Great Minds Think Alike',\n",
       " 'GN': 'Good Night',\n",
       " 'GOAT': 'Greatest Of All Time',\n",
       " 'GR8': 'Great!',\n",
       " 'HBD': 'Happy Birthday',\n",
       " 'IC': 'I See',\n",
       " 'ICQ': 'I Seek You',\n",
       " 'IDC': 'I Donâ€™t Care',\n",
       " 'IDK': \"I Don't Know\",\n",
       " 'IFYP': 'I Feel Your Pain',\n",
       " 'ILU': 'I Love You',\n",
       " 'ILY': 'I Love You',\n",
       " 'IMHO': 'In My Honest/Humble Opinion',\n",
       " 'IMU': 'I Miss You',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'IOW': 'In Other Words',\n",
       " 'IRL': 'In Real Life',\n",
       " 'IYKYK': 'If You Know, You Know',\n",
       " 'JK': 'Just Kidding',\n",
       " 'KISS': 'Keep It Simple, Stupid',\n",
       " 'L': 'Loss',\n",
       " 'L8R': 'Later',\n",
       " 'LDR': 'Long Distance Relationship',\n",
       " 'LMK': 'Let Me Know',\n",
       " 'LMAO': 'Laughing My A** Off',\n",
       " 'LOL': 'Laughing Out Loud',\n",
       " 'LTNS': 'Long Time No See',\n",
       " 'M8': 'Mate',\n",
       " 'MFW': 'My Face When',\n",
       " 'MID': 'Mediocre',\n",
       " 'MRW': 'My Reaction When',\n",
       " 'MTE': 'My Thoughts Exactly',\n",
       " 'NVM': 'Never Mind',\n",
       " 'NRN': 'No Reply Necessary',\n",
       " 'NPC': 'Non-Player Character',\n",
       " 'OIC': 'Oh I See',\n",
       " 'OP': 'Overpowered',\n",
       " 'PITA': 'Pain In The A**',\n",
       " 'POV': 'Point Of View',\n",
       " 'PRT': 'Party',\n",
       " 'PRW': 'Parents Are Watching',\n",
       " 'ROFL': 'Rolling On The Floor Laughing',\n",
       " 'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
       " 'ROTFLMAO': 'Rolling On The Floor Laughing My A** Off',\n",
       " 'RN': 'Right Now',\n",
       " 'SK8': 'Skate',\n",
       " 'STATS': 'Your Sex And Age',\n",
       " 'SUS': 'Suspicious',\n",
       " 'TBH': 'To Be Honest',\n",
       " 'TFW': 'That Feeling When',\n",
       " 'THX': 'Thank You',\n",
       " 'TIME': 'Tears In My Eyes',\n",
       " 'TLDR': 'Too Long, Didnâ€™t Read',\n",
       " 'TNTL': 'Trying Not To Laugh',\n",
       " 'TTFN': 'Ta-Ta For Now!',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'U': 'You',\n",
       " 'U2': 'You Too',\n",
       " 'U4E': 'Yours For Ever',\n",
       " 'W': 'Win',\n",
       " 'W8': 'Wait...',\n",
       " 'WB': 'Welcome Back',\n",
       " 'WTF': 'What The F**k',\n",
       " 'WTG': 'Way To Go!',\n",
       " 'WUF': 'Where Are You From?',\n",
       " 'WYD': 'What You Doing?',\n",
       " 'WYWH': 'Wish You Were Here',\n",
       " 'ZZZ': 'Sleeping, Bored, Tired'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d23e5c1d-4030-4ff2-89a6-153c58cbc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e00f604-81a6-43c0-a091-3a966d1bc9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are You , Good Night'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversion(\"how are u , gn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431532e-2d35-447b-b438-c1641ef04fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b87dd5f3-c63a-4f8a-b2cc-6123df72bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d957a601-f387-439e-9329-4aff0c16cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "104310e6-3867-492c-91b0-9122fed8ee0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' certain  example example generation data science'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text=\" ceertain  example exampla genratio data scienc\"\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fb6cf-fffc-460a-a45e-aa26626d04fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37a0c65b-b628-46cb-905c-b27b9cbe51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd878bf0-ea50-4c45-9d42-2df31e6a5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b967bde-899f-4903-9abb-2160248d7650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87d03b3b-40c0-4275-8d79-dda9eb84331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08d67d1b-8575-403b-939f-0052cd2753a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello   rahul    ?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"hello i am rahul who are you ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64953666-653d-4033-a912-00daa4632089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    one    reviewers  mentioned   watching  1 oz e...\n",
       "1     wonderful little production.  filming techniq...\n",
       "2     thought    wonderful way  spend time    hot s...\n",
       "3    basically there's  family   little boy (jake) ...\n",
       "4    petter mattei's \"love   time  money\"   visuall...\n",
       "5    probably  all-time favorite movie,  story  sel...\n",
       "6     sure would like  see  resurrection    dated s...\n",
       "7     show   amazing, fresh & innovative idea   70'...\n",
       "8    encouraged   positive comments   film     look...\n",
       "9      like original gut wrenching laughter   like ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"].head(10).apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d76752-50da-4a67-96ec-f561f238d1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3a3f7cfb-cb14-4e0e-9ba5-c7129d97bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\" \n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\u2702-\\u27B0\"          # misc symbols\n",
    "        u\"\\u24C2-\\U0001F251\"      # enclosed characters\n",
    "        \"]+\", \n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29df5127-632f-4c47-89f0-b45a5466661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was  '"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was  ðŸ˜„ðŸ˜„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eab4f825-1fac-403f-8252-67ded9364055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Learning Python is fun '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Learning Python is fun ðŸ˜„ðŸš€ðŸ”¥ðŸ“š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e044480-c312-4f94-8940-dae20be070c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/590.6 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/590.6 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/590.6 kB 217.9 kB/s eta 0:00:03\n",
      "   ---- ---------------------------------- 61.4/590.6 kB 297.7 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 112.6/590.6 kB 467.6 kB/s eta 0:00:02\n",
      "   ------- ------------------------------ 112.6/590.6 kB 467.6 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 245.8/590.6 kB 793.0 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 245.8/590.6 kB 793.0 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 368.6/590.6 kB 882.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 501.8/590.6 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  583.7/590.6 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 590.6/590.6 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "03628da1-e8f4-4211-983b-9fd0bab67497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d9719d58-3ade-4110-80a5-87679069fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was  :grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize(\"Loved the movie. It was  ðŸ˜„ðŸ˜„\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ffc9ef-644b-4e32-85ac-96f69bb31ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fb329-1ab6-4b88-a8fa-aca0e565304f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e3439d-9f38-4ac8-9017-419529190f5c",
   "metadata": {},
   "source": [
    "1. Using the split tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e11d4425-098e-4e99-9d51-8a84b99a0fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenization\n",
    "\n",
    "sent1 = \"I am going to delhi . \"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b2f6100f-437d-410e-a843-80724d170660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' I am going to delhi ',\n",
       " ' I will stay there for 3 days',\n",
       " \"let's hope the trip to be great\",\n",
       " '']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sent2 = \" I am going to delhi . I will stay there for 3 days.let's hope the trip to be great.\"\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "beef7fdf-9359-4b1b-96c3-58789e36d5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem with split function\n",
    "\n",
    "sent3 = \"I am going to delhi!\"\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f2a4f898-71cd-43bb-8ef6-15417edadabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where do think I shoud go? I have 3 days holidays']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4= \"where do think I shoud go? I have 3 days holidays\"\n",
    "sent4.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d81aa-6f3f-4b82-8e69-e776db483420",
   "metadata": {},
   "source": [
    "2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "13300c2b-fb2a-462f-a218-de2882abfbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent3 = \"I am going to delhi!\"\n",
    "tokens = re.findall(\"[\\\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4a307fa2-add0-46a0-bbbb-81d249a0a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem Ipsum is simply dummy text of the printing and typesetting industry', \" \\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book\", '']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sentences = re.compile('[.!?]').split(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509fa8b-03a9-42de-be8c-4d08c855eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be422c62-9fc5-49d6-92c9-4bbdb25d5360",
   "metadata": {},
   "source": [
    "3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a43931bd-2797-4dd8-891b-1c958ed64d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4ce8d0b0-46ce-4011-9cc9-f9509b5c1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5d72a9c2-8f42-40f4-9d38-13df144135a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen1 = \"I am going to visit delhi!\"\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "55419f27-f9ee-4d1d-9bb8-9d2b5a50ac55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0f26eebf-ff2e-4366-9ea6-af15aeaae55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4 = \" I have a ph.D in AI\"\n",
    "sent5 = \" We are here to help! mail us at nks@gmail.com\"\n",
    "sent6 = \" A 4km ride cost $10.50\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f9ca1102-0031-4f09-b373-142c3716276a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '4km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cee373a2-49a7-4c61-9a25-414ac8b29573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a11b8d03-6cbc-46d6-b56c-20ae17b2ed87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'ph.D', 'in', 'AI']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9761cf-56ad-4039-b773-5caa32e7ae2b",
   "metadata": {},
   "source": [
    "4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c9cc9673-117b-4526-ae40-901bbc7ed96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.3.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB 8.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.3/13.9 MB 16.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 16.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.9/13.9 MB 10.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.5/13.9 MB 11.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.0/13.9 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.8/13.9 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 6.3/13.9 MB 17.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.8/13.9 MB 19.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.0/13.9 MB 19.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 10.3/13.9 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.6/13.9 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.9/13.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.9/13.9 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/13.9 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 23.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 183.0/183.0 kB 10.8 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.7/116.7 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------  624.6/632.6 kB 38.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 632.6/632.6 kB 19.4 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 25.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 27.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.3 MB/s eta 0:00:00\n",
      "Using cached numpy-2.3.2-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.3/46.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.0/6.3 MB 20.0 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.3/6.3 MB 16.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 14.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.3/6.3 MB 14.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.9/6.3 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.4/6.3 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.9/6.3 MB 13.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 13.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 12.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 52.8/52.8 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.4/5.4 MB 13.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.8/5.4 MB 10.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.2/5.4 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.6/5.4 MB 9.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.0/5.4 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.4/5.4 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.8/5.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.7/5.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "   ---------------------------------------- 0.0/150.8 kB ? eta -:--:--\n",
      "   -------------------------------------- - 143.4/150.8 kB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 143.4/150.8 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.8/150.8 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 numpy-2.3.2 preshed-3.0.10 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.2 which is incompatible.\n",
      "numba 0.59.1 requires numpy<1.27,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.3.2 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.2 which is incompatible.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.3.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7a75cb29-d3d8-4f9a-b93e-5669827b460e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.5.3)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/68.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/68.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/68.0 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 30.7/68.0 kB 163.8 kB/s eta 0:00:01\n",
      "     ---------------------- --------------- 41.0/68.0 kB 178.6 kB/s eta 0:00:01\n",
      "     ---------------------------------- --- 61.4/68.0 kB 233.8 kB/s eta 0:00:01\n",
      "     ---------------------------------- --- 61.4/68.0 kB 233.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 68.0/68.0 kB 194.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic) (0.6.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "   ---------------------------------------- 0.0/444.8 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/444.8 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 112.6/444.8 kB 1.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 225.3/444.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/444.8 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  440.3/444.8 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 444.8/444.8 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.4/2.0 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 13.9 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "   ---------------------------------------- 0.0/43.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 43.9/43.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-extensions, typing-inspection, pydantic-core, pydantic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.14.6\n",
      "    Uninstalling pydantic_core-2.14.6:\n",
      "      Successfully uninstalled pydantic_core-2.14.6\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.3\n",
      "    Uninstalling pydantic-2.5.3:\n",
      "      Successfully uninstalled pydantic-2.5.3\n",
      "Successfully installed pydantic-2.11.7 pydantic-core-2.33.2 typing-extensions-4.14.1 typing-inspection-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires numpy<2,>=1.19.3, but you have numpy 2.3.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ab3109d9-7c15-4d50-87fb-b24784fcd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.11.7)\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"pydantic>=2.0\" spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "942bf137-bf21-437e-948b-9cb6a2fc4e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[199], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     ConfigSchema,\n\u001b[0;32m     49\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     validate_init_settings,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[0;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d1699cd2-8e6c-45a4-b720-4135b7ce94ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.11.7)\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic>=2.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"pydantic>=2.0\" spacy\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb11d9-ff03-411c-b57f-5561ae414aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(sent5)\n",
    "doc2=nlp(sent6)\n",
    "doc3=nlp(sent4)\n",
    "doc4=nlp(sent1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "49fc92ad-6154-4465-ac31-3465f29f08f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world, this is a test.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     ConfigSchema,\n\u001b[0;32m     49\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     validate_init_settings,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[0;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello world, this is a test.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bef139-998c-4480-8f3c-9d8097c6d3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b901eb-5db6-4390-bbcb-880f58e33bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd95253-1954-4c7b-83e2-ed05eea5818e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0af69a-6b90-4c8d-afc6-992f55872f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e839bb-733f-4ce6-9e5b-3a2a33dc8bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff941b2f-2c09-4d54-bb03-b94294242da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14abbc3-d3bc-47f0-a9d3-190cc0e7e572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "368c6f0f-3843-4955-9059-72353484145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import  PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b0bc1a44-a89a-4904-9888-bb085e2c87d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word)for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "43a16173-ca0d-4b02-ae04-da038a182819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d883e868-711c-4811-aa9e-c5e31651e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its \n",
    "not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years \n",
    "paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a \n",
    "delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch\n",
    "and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had \n",
    "a dozen thumbs theyd all be up for this movie\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "97837ab2-c2cd-481d-a907-768ead7a6c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cd050-f78f-4908-afef-4fdb7c91e7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
